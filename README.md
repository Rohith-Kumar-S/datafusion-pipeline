# DataFusion: Real-Time Data Integration Pipeline

A comprehensive rule-based data integration platform that enables users to build and execute scalable data processing pipelines through an intuitive user interface. The system supports both real-time streaming and batch processing with distributed computing capabilities.

## ğŸš€ Features

### Core Capabilities

- **Rule-Based Processing**: User-friendly interface for defining data transformations without coding
- **Real-Time Streaming**: Apache Kafka integration for continuous data processing
- **Data Fusion**: Intelligent joining of datasets from multiple sources
- **Parallel Execution**: Multi-threaded pipeline execution with concurrent processing
- **Flexible Input Sources**: Support for Kaggle datasets, GitHub repositories, and Kafka streams
- **Multiple Output Destinations**: Local files, console logging, and RDBMS integration


### Technical Highlights

- **Distributed Computing**: Powered by Apache Spark for scalable data processing
- **Containerized Deployment**: Docker-based architecture for consistent environments
- **NoSQL Metadata Management**: MongoDB for storing rules, configurations, and pipeline metadata
- **Interactive Dashboard**: Streamlit-based user interface for configuration and monitoring


## ğŸ—ï¸ Architecture

The system follows a modular microservices architecture with the following components:

```
DataFusion Pipeline Architecture
â”œâ”€â”€ Data Ingestion Layer (Kafka + File Processing)
â”œâ”€â”€ Processing Engine (PySpark + Rule Engine)
â”œâ”€â”€ Data Fusion Layer (Multi-source joining)
â”œâ”€â”€ Output Management (Local/RDBMS/Console)
â””â”€â”€ User Interface (Streamlit Dashboard)
```


### Workflow

1. **Initialization**: Spark session creation and MongoDB connection
2. **Configuration**: Users define input sources, rules, fusion operations, and targets
3. **Pipeline Building**: Components are linked with hierarchical relationships
4. **Execution**: Parallel processing with persistent streaming support

## ğŸ“‹ Prerequisites

- Docker and Docker Compose
- Python 3.8+
- At least 4GB RAM recommended
- Network access for external data sources


## ğŸ› ï¸ Installation \& Setup

### Quick Start with Docker

1. **Clone the repository**
```bash
git clone <repository-url>
cd datafusion
```

2. **Launch the application**
```bash
docker-compose up -d
```

3. **Access the application**

- Open your browser and navigate to `http://localhost:8501`
- The Streamlit interface will load with configuration pages


### Manual Setup

1. **Install dependencies**
```bash
cd datafusion
pip install -r requirements.txt
```

2. **Start MongoDB** (ensure MongoDB is running locally or update connection string)
3. **Run the application**
```bash
streamlit run app.py
```


## ğŸ“ Project Structure

```
datafusion/
â”œâ”€â”€ data/                          # Data storage directory
â”œâ”€â”€ dependencies/                  # External dependencies and libraries
â”œâ”€â”€ pages/                        # Streamlit application pages
â”‚   â”œâ”€â”€ 1_data_ingestion.py       # Input source configuration
â”‚   â”œâ”€â”€ 2_data_processor.py       # Rule engine interface
â”‚   â”œâ”€â”€ 3_data_unifier.py         # Data fusion configuration
â”‚   â”œâ”€â”€ 4_data_sinker.py          # Output destination setup
â”‚   â”œâ”€â”€ 5_pipeline_builder.py     # Pipeline assembly interface
â”‚   â””â”€â”€ 6_pipeline_executer.py    # Pipeline execution and monitoring
â”œâ”€â”€ app.py                        # Main Streamlit application
â”œâ”€â”€ Dockerfile                    # Container configuration
â”œâ”€â”€ pipeline_engine.py            # Core pipeline processing logic
â”œâ”€â”€ pipeline_utils.py             # Utility functions and helpers
â””â”€â”€ requirements.txt              # Python dependencies

docker-compose.yml                # Multi-container orchestration
README.md                        # Project documentation
```


## ğŸ”§ Usage

### 1. Configure Input Sources

- Navigate to **Data Ingestion** page
- Add data sources (Kaggle links, GitHub repositories, or Kafka topics)
- Configure connection parameters and data schemas


### 2. Define Processing Rules

- Go to **Data Processor** page
- Create transformation rules using available operations:
    - **Cast**: Data type conversion
    - **Rename**: Rename a column
    - **Filter**: Row-level data selection
    - **Drop**: Remove unnecessary columns, null values and duplicates
    - **Fill**: Remove columns, and null values
    - **Explode**: Handle nested data structures
    - **Flatten**: Normalize complex nested data
    - **Save**: Persist intermediate results


### 3. Set Up Data Fusion

- Access **Data Unifier** page
- Configure join operations between different data sources
- Define join keys and relationship types


### 4. Configure Output Destinations

- Visit **Data Sinker** page
- Set up output targets:
    - Local file downloads
    - Console logging
    - RDBMS tables (via JDBC)


### 5. Build and Execute Pipelines

- Use **Pipeline Builder** to assemble components
- Execute pipelines through **Pipeline Executer**
- Monitor real-time processing status and logs


## ğŸ”„ Data Processing Operations

The rule engine supports the following transformation operations:


| Operation | Description | Use Case |
| :-- | :-- | :-- |
| **Cast** | Convert data types | String to numeric conversion |
| **Rename** | Rename a column | Rename a column |
| **Filter** | Apply conditional filtering | Remove invalid records |
| **Drop** | Remove columns | Schema simplification |
| **Fill** | Fill columns |Fill null values |
| **Explode** | Expand nested structures | JSON/XML data processing |
| **Flatten** | Normalize nested data | API response processing |
| **Save** | Persist intermediate data | Checkpoint creation |

## ğŸ”— Integration Examples

### Kafka Streaming Integration

```python
# Example Kafka topic configuration
topic_name = "iot-sensor-data"
kafka_servers = "localhost:9092"
```


### Database Output Configuration

```python
# RDBMS connection example
jdbc_url = "jdbc:postgresql://localhost:5432/dbtablename"
table_name = "processed_sensor_data"
```


## ğŸš¦ Pipeline Execution Modes

### Batch Processing

- Processes complete datasets at scheduled intervals
- Optimal for large historical data analysis
- Terminates upon completion


### Stream Processing

- Continuous real-time data processing
- Persistent execution (survives application restarts)
- Low-latency data transformation


## ğŸ›¡ï¸ Error Handling \& Monitoring

- **Real-time Status**: Live pipeline execution monitoring
- **Error Logging**: Comprehensive error tracking and reporting
- **Data Quality**: Built-in validation and quality checks
- **Recovery**: Automatic retry mechanisms for failed operations


## ğŸ§ª Testing with Sample Data

The project includes sample IoT datasets for testing:

- Customer data (CI00100-CI00150)
- Device information (D000-D050)
- Sensor readings with temperature, status, and timestamp data


## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-feature`)
3. Commit your changes (`git commit -am 'Add new feature'`)
4. Push to the branch (`git push origin feature/new-feature`)
5. Create a Pull Request

## ğŸ”§ Technologies Used

- **Backend**: Apache Spark (PySpark), Python 3.8+
- **Streaming**: Apache Kafka
- **Database**: MongoDB
- **Frontend**: Streamlit
- **Containerization**: Docker, Docker Compose
- **Data Processing**: Pandas, NumPy

**DataFusion** - Democratizing data integration through intelligent automation and user-friendly interfaces.

