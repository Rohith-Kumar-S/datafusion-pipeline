{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36fb2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d504e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302ed433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    \"\"\"Read a CSV file and return a DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b63d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db(db_path):\n",
    "    \"\"\"Connect to a SQLite database and return the connection object.\"\"\"\n",
    "    global db_connected\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    db_connected = True\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eac156c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_dataframes(dfs):\n",
    "    \"\"\"Concatenate a list of DataFrames into a single DataFrame.\"\"\"\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06f7790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_kaggle_data(dataset):\n",
    "    \"\"\"Import a dataset from Kaggle and return the DataFrame.\"\"\"\n",
    "    print(f\"Importing dataset: {dataset}\")\n",
    "    path = kagglehub.dataset_download(dataset)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30bd8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"https://www.kaggle.com/datasets/dakshbhatnagar08/global-high-school-student-lifestyle-and-wellnesshttps://www.kaggle.com/datasets/skullagos5246/upi-transactions-2024-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9373021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not a kaggle link: \n",
      "Importing dataset: dakshbhatnagar08/global-high-school-student-lifestyle-and-wellness\n",
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n",
      "Importing dataset: skullagos5246/upi-transactions-2024-dataset\n",
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n"
     ]
    }
   ],
   "source": [
    "res = str.split(r'https://')\n",
    "datasets = {}\n",
    "for i in res:\n",
    "    if 'kaggle' in i.strip() and 'datasets' in i.strip():\n",
    "        dataset_name = i.strip().split('datasets/')[1]\n",
    "        datasets[dataset_name] = None\n",
    "        datasets[dataset_name] = import_kaggle_data(dataset_name)\n",
    "    else:\n",
    "        print('not a kaggle link:', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79308324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dakshbhatnagar08/global-high-school-student-lifestyle-and-wellness': 'C:\\\\Users\\\\rohit\\\\.cache\\\\kagglehub\\\\datasets\\\\dakshbhatnagar08\\\\global-high-school-student-lifestyle-and-wellness\\\\versions\\\\1',\n",
       " 'skullagos5246/upi-transactions-2024-dataset': 'C:\\\\Users\\\\rohit\\\\.cache\\\\kagglehub\\\\datasets\\\\skullagos5246\\\\upi-transactions-2024-dataset\\\\versions\\\\2'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7bbfd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sudo is disabled on this machine. To enable it, go to the \u001b]8;;ms-settings:developers\u001b\\Developer Settings page\u001b]8;;\u001b\\ in the Settings app\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'spark-3.2.1-bin-hadoop3.2.tgz'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\rohit\\anaconda3\\envs\\cv_env_20250327\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\rohit\\anaconda3\\envs\\cv_env_20250327\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Requirement already satisfied: py4j in c:\\users\\rohit\\anaconda3\\envs\\cv_env_20250327\\lib\\site-packages (0.10.9.9)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mT\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m     27\u001b[39m spark= SparkSession \\\n\u001b[32m     28\u001b[39m        .builder \\\n\u001b[32m     29\u001b[39m        .appName(\u001b[33m\"\u001b[39m\u001b[33mOur First Spark Example\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m        .getOrCreate()\n\u001b[32m     32\u001b[39m spark\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rohit\\anaconda3\\envs\\cv_env_20250327\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = SparkContext.getOrCreate(sparkConf)\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rohit\\anaconda3\\envs\\cv_env_20250327\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         SparkContext(conf=conf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rohit\\anaconda3\\envs\\cv_env_20250327\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rohit\\anaconda3\\envs\\cv_env_20250327\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rohit\\anaconda3\\envs\\cv_env_20250327\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "!pip install pyspark\n",
    "!pip install py4j\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from typing import List\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark= SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"Our First Spark Example\") \\\n",
    "       .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097dfd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79242fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"COVID SQL Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7515dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8091a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\rohit\\.cache\\kagglehub\\datasets\\thedevastator\\unlock-profits-with-e-commerce-sales-data\\versions\\2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all csv, .json,.xlsx files in the directory\n",
    "\n",
    "def find_files(directory, extensions=['.csv', '.json', '.xlsx']):\n",
    "    \"\"\"Find all files in the directory with the specified extensions.\"\"\"\n",
    "    files = []\n",
    "    for root, dirs, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            if any(filename.endswith(ext) for ext in extensions):\n",
    "                files.append(os.path.join(root, filename))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2cf574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = ['.csv', '.json', '.xlsx']\n",
    "files = find_files(path, extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f216a6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\rohit\\\\.cache\\\\kagglehub\\\\datasets\\\\thedevastator\\\\unlock-profits-with-e-commerce-sales-data\\\\versions\\\\2\\\\Amazon Sale Report.csv',\n",
       " 'C:\\\\Users\\\\rohit\\\\.cache\\\\kagglehub\\\\datasets\\\\thedevastator\\\\unlock-profits-with-e-commerce-sales-data\\\\versions\\\\2\\\\Cloud Warehouse Compersion Chart.csv',\n",
       " 'C:\\\\Users\\\\rohit\\\\.cache\\\\kagglehub\\\\datasets\\\\thedevastator\\\\unlock-profits-with-e-commerce-sales-data\\\\versions\\\\2\\\\Expense IIGF.csv',\n",
       " 'C:\\\\Users\\\\rohit\\\\.cache\\\\kagglehub\\\\datasets\\\\thedevastator\\\\unlock-profits-with-e-commerce-sales-data\\\\versions\\\\2\\\\International sale Report.csv',\n",
       " 'C:\\\\Users\\\\rohit\\\\.cache\\\\kagglehub\\\\datasets\\\\thedevastator\\\\unlock-profits-with-e-commerce-sales-data\\\\versions\\\\2\\\\May-2022.csv',\n",
       " 'C:\\\\Users\\\\rohit\\\\.cache\\\\kagglehub\\\\datasets\\\\thedevastator\\\\unlock-profits-with-e-commerce-sales-data\\\\versions\\\\2\\\\P  L March 2021.csv',\n",
       " 'C:\\\\Users\\\\rohit\\\\.cache\\\\kagglehub\\\\datasets\\\\thedevastator\\\\unlock-profits-with-e-commerce-sales-data\\\\versions\\\\2\\\\Sale Report.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945ebb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "match = re.search(r'\\d+', \"123abc456\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "534a02ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='123'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a610df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(r'[^\\d]+[a-zA-Z]*(\\d+)?', \"123abc12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9b519b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc12'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match.group(0) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d3ab4b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p_le_.csv'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\s+', '_', \"p  le .csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# replace all whitespace characters with underscores\n",
    "def replace_whitespace_with_underscore(text):\n",
    "    \"\"\"Replace all whitespace characters in the text with underscores.\"\"\"\n",
    "    return re.sub(r'\\s+', '_', text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env_20250327",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
